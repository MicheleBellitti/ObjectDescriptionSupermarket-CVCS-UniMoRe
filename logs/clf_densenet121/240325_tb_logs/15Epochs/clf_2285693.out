Starting clf_test.py 

Inference images and results saved.
Starting clf_evaluate.py 

Classification Report:
               precision    recall  f1-score   support

           0      0.805     0.989     0.888       276
           1      0.950     0.950     0.950        40
           2      0.977     0.955     0.966        44
           3      1.000     0.911     0.953        45
           4      0.727     0.585     0.649        41
           5      0.600     0.300     0.400        30
           6      0.792     0.613     0.691        31
           7      0.972     0.895     0.932       153
           8      0.864     0.543     0.667        35
           9      0.425     0.554     0.481        56
          10      0.618     1.000     0.764        21
          11      0.909     0.741     0.816        27
          12      1.000     0.944     0.971        36
          13      0.955     0.787     0.863       108
          14      0.958     0.920     0.939        25
          15      1.000     0.909     0.952        22
          16      1.000     0.920     0.958        25
          17      0.452     0.412     0.431        34
          18      0.691     0.559     0.618        68
          19      1.000     0.995     0.998       219
          20      0.988     1.000     0.994       164
          21      1.000     0.967     0.983        30
          22      1.000     1.000     1.000        31
          23      1.000     0.829     0.907        41
          24      1.000     1.000     1.000        19
          25      1.000     1.000     1.000        47
          26      1.000     1.000     1.000        58
          27      0.940     1.000     0.969       172
          28      0.650     0.929     0.765        14
          29      0.917     1.000     0.957        22
          30      1.000     1.000     1.000        19
          31      0.976     0.952     0.964        42
          32      0.950     0.704     0.809        27
          33      0.926     1.000     0.962        25
          34      0.917     0.733     0.815        15
          35      0.905     0.905     0.905        21
          36      0.929     1.000     0.963        39
          37      0.947     0.973     0.960        37
          38      0.956     0.982     0.969       110
          39      0.971     0.943     0.957        70
          40      0.895     1.000     0.944        17
          41      0.960     0.970     0.965       100
          42      0.844     0.931     0.885        29

    accuracy                          0.902      2485
   macro avg      0.892     0.867     0.873      2485
weighted avg      0.907     0.902     0.900      2485

